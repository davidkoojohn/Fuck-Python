## Robots协议，全称是“网络爬虫排除标准”

## 法不禁止即为许可

```
# Robots.txt文件

User-agent:  Baiduspider
Allow:  /article
Allow:  /oshtml
Disallow:  /product/
Disallow:  /

User-Agent:  *
Disallow:  /
```

* BuiltWith：识别网站所用技术的工具
* python-whois：查询网站所有者的工具
* robotparser：解析robots.txt的工具

> 一个基本的爬虫通常分为三个部分的内容

1. 数据采集（网页下载）、
2. 数据处理（网页解析）和
3. 数据存储（将有用的信息持久化）

更为高级的爬虫在数据采集和处理时会使用并发编程或分布式技术

一般来说，爬虫的工作流程包括以下几个步骤：

1. 设定抓取目标（种子页面/起始页面）并获取网页。
2. 当服务器无法访问时，按照指定的重试次数尝试重新下载页面。
3. 在需要的时候设置用户代理或隐藏真实IP，否则可能无法访问页面。
4. 对获取的页面进行必要的解码操作然后抓取出需要的信息。
5. 在获取的页面中通过某种方式（如正则表达式）抽取出页面中的链接信息。
6. 对链接进行进一步的处理（获取页面并重复上面的动作）。
7. 将有用的信息进行持久化以备后续的处理。



